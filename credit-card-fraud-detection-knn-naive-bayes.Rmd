---
title: "Credit Card Fraud Detection via KNN and Naive Bayes classifiers"
author: "Yura Shakhnazaryan"
date: '11 12 2017'
output: html_document
---

```{r setup, include=FALSE}
library(caret)
library(class)
library(e1071)
library(ROCR)
credit <- read.csv("../input/creditcard.csv")
```

The primary purpose of this kernel is to perform K-Nearest Neighbours and Naive Bayes classifying algorithms on the "Credit Card Fraud Detection" dataset" and 
discover the highest accuracy model between the two. 

First things first, the structure of the dataset was immediately looked upon:

```{r}
str(credit)
```

Because the variable "Class" is of a class "integer", the factor transformation was performed: 

```{r}
credit$Class <- factor(credit$Class)
```

Since the data was tremendously overwhelming in terms of its number of observations and considering that the kernel is run out of nothing else but scientific 
curiosity, I decided to work with the 20% sized sample. Next, training and testing objects were generated, while accounting for the effect of randomness.

```{r}
set.seed(1998)
samp <- sample(1:nrow(credit), round(0.2*nrow(credit)))
credit <- credit[samp, ]
index <- createDataPartition(credit$Class, p = 0.75, list = F)
train <- credit[index, ]
test <- credit[-index, ]
```

# K-Nearest Neighbors

As all the variables but that of prediction were of class either "numeric" or "integer", whereas the data itself was of a scaled format, I proceeded to performing 
the knn classification right away. The number of neighbours was set to 5 as a default. 

```{r}
knn1 <- knn(train = train[,-31], test = test[,-31], cl = train$Class, k = 5)
confusionMatrix(knn1, test$Class, positive = "1")
```

Apparently, a 99.8% accuracy was obtained with the specified index of "k", yet still there were few drawbacks with the output from the "confusionMatrix". 

One of such drawbacks was that the model did not predict any cases of "fraud" (that is, "Class" = 1), though some actual cases of fraud were erroneously assigned to 
be non-fraud. 

The other drawback laid in the fact that the achieved accuracy did not exceed the No-Information rate (if all the observations were predicted to be non-fraud). 
Why the two measures were equivalent directly comes from what has been mentioned above: all the cases were assigned to be non-fraud. 

Overall, at this juncture it is challenging to once and for all state whether employing knn is a worthwhile endeavor for obtaining the highest possible accuracy. 
The subsequent analysis will, thus, explore that part in more details. 

# Naive Bayes

To argue even further on the overall effectiveness of the knn-based results, I referred to another popular classifier, widely regarded as "Naive Bayes" model. 
The modeling was set according to all the standards, adjusting for the possibility of experiencing posterior class probability of "0" by "laplace = 1".

```{r}
bayes <- naiveBayes(Class~., data = train, laplace = 1)
bayes$apriori
```

The apriori frequencies suggest that the model is, again, heavily inclined to assigning non-fraud values to the observations, given the prevalence of these in 
the dataset.

To observe a true winner between the two classifiers, I generated the predicted classes for a testing dataset and approached the very same confusionMatrix: 

```{r}
pred <- predict(bayes, test)
confusionMatrix(pred, test$Class, positive = "1")
```

As pinpointed in the above table, the naive Bayes algorithm, in contrast to its counterpart in K-Nearest Neighbors, does tend to consider certain, suitable 
observations fraud. Nevertheless, the model resulted in a worse accuracy rate, which, furthermore, is, again, lower than the benchmark of No-Information rate.

To either become fully convinced of the fact that naive Bayes is an underperforming algorithm in comparison with knn or, other way around, doubt such a reasoning, 
I settled on deriving "raw" bayesian probabilities and measuring the performance, this time via the available options from the package "ROCR":

```{r}
rawpred <- predict(bayes, test, type = "raw")
ptest <- prediction(rawpred[,2], test$Class)
perf <- performance(ptest, "tpr", "fpr")
plot(perf, colorize = T)
performance(ptest, "auc")@y.values
```

Sadly for the worshippers of a Bayesian cult, the area under curve indicator for the "naive" model proved even weaker than the accuracy attained under the 
confusionMatrix specification. 

# Conclusion

To sum up, with regards to our dataset, K-Nearest Neighbors algorithm clearly should be prioritized over Naive Bayes in the extraction of the most accurate 
predictions in terms of whether a credit card will be detected fraud or not, save for other, perhaps more effective, classifiers. 

While, paradoxically enough, both almost perfect and somewhat lackluster, knn outperforms Bayesian instrument by a fair margin and, hence, is your starting choice 
for similar studies in the future.